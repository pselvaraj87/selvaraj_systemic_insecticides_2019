import os
import pandas as pd
import numpy as np
from calibtool.CalibSite import CalibSite

from dtk.vector.species import set_species_param
from helpers.relative_time import convert_to_day_365

from helpers.windows_filesystem import *

from gridded_sims.calib.LL_analyzers import *

dropbox = get_dropbox_location()
project_folder = os.path.join(dropbox, "projects/mz_magude/")

# Grid-cell/Node ID
def find_cells_for_this_catchment(catch):
    # Find which grid cells correspond to a given HFCA
    df = pd.read_csv(os.path.join(project_folder, "dtk_simulation_input/mozambique/grid_lookup_with_neighborhood.csv"))

    if catch == 'all':
        return np.array(df['grid_cell'])
    else:
        df_catch = df[df['catchment'] == catch]
        return np.array(df_catch['grid_cell'])


def find_pops_for_catch(catch):
    cells = find_cells_for_this_catchment(catch)

    pop_df = pd.read_csv(os.path.join(project_folder, "dtk_simulation_input/mozambique/grid_population.csv"))

    return np.array(pop_df[np.in1d(pop_df['node_label'], cells)]['pop'])


def find_bairros_for_this_catchment(catch):
    bairro_data = pd.read_csv(os.path.join(project_folder, "dtk_simulation_input/mozambique/grid_lookup_with_neighborhood.csv"))

    return_data = bairro_data[bairro_data["catchment"] == catch]
    return return_data


def catch_3_yr_spline(catch, species):
    # Read spline directly from mini-CSV files generated by Jaline/Caitlin

    ento_base = os.path.join(project_folder, "entomology_calibration/Multi_year_calibration_by_HFCA_180808/minicsv")

    def load_raw_spline(csv_df):
        spline = np.zeros(36)
        raw_spline = np.array(csv_df["Values"])
        spline[4:35] = raw_spline[1:-1]  # Throw out first and last entry
        return spline

    def fill_out_spline(spline):
        spline[0:4] = 0.5 * (spline[12:16] + spline[24:28])
        spline[35] = 0.5 * (spline[11] + spline[23])
        return spline

    # def fill_out_spline(spline):
    #     # Account for the fact that we only really trust the data from indices 4-35 (May 15 - Nov 17)
    #     # Reconstruct shape and amplitude of missing data by inferring from other
    #     year1_factor = spline[4:11].sum()
    #     year2_factor = spline[16:23].sum()
    #     year3_factor = spline[28:35].sum()
    #
    #     spline_inferred = spline.copy()
    #
    #     spline_inferred[0:4] = year1_factor * 0.5 * (spline[12:16] / year2_factor + spline[24:28] / year3_factor)
    #     spline_inferred[35] = year3_factor * 0.5 * (spline[11] / year1_factor + spline[23] / year2_factor)
    #
    #     return spline_inferred


    if species == "arabiensis":
        species = "gambiae"

    if species == "funestus":
        df = pd.read_csv(os.path.join(ento_base, "Three_funestus_LifeAdj_rank0.csv"))
        spline = fill_out_spline(load_raw_spline(df))
    elif species == "gambiae":
        if catch == "Panjane-Caputine":
            df = pd.read_csv(os.path.join(ento_base, "Panjane_gambiae_frankenspline.csv"))
            spline = fill_out_spline(load_raw_spline(df))
        elif catch == "Magude-Sede-Facazissa":
            df = pd.read_csv(os.path.join(ento_base, "Magude-Sede_gambiae_LifeAdj_rank0.csv"))
            spline = fill_out_spline(load_raw_spline(df))
        elif catch != "Moine" and catch != "Mahel":
            df = pd.read_csv(os.path.join(ento_base, "{}_{}_frankenspline.csv".format(catch, species)))
            spline = fill_out_spline(load_raw_spline(df))
        elif catch == "Moine" or catch == "Mahel":
            panjane_df = pd.read_csv(os.path.join(ento_base, "Panjane_gambiae_frankenspline.csv"))
            chichuco_df = pd.read_csv(os.path.join(ento_base, "Chichuco_gambiae_frankenspline.csv"))
            panjane_spline = fill_out_spline(load_raw_spline(panjane_df))
            chichuco_spline = fill_out_spline(load_raw_spline(chichuco_df))

            spline = (panjane_spline + chichuco_spline) / 2.

    # Return associated times:
    times_1yr = np.array(
        [0.0, 30.417, 60.833, 91.25, 121.667, 152.083, 182.5, 212.917, 243.333, 273.75, 304.167, 334.583])
    times = np.append(times_1yr, times_1yr + 365)
    times = np.append(times, times_1yr + 365 * 2)
    times = list(times)

    return [times, list(spline)]



def set_x_local_migration(cb, catch):
    xdict = {"Chichuco": 0.8,
             "Chicutso": 2.0,
             "Magude-Sede-Facazissa": 0.4,
             "Mahel": 5.0,
             "Mapulanguene": 5.0,
             "Moine": 1.5,
             "Motaze": 0.6,
             "Panjane-Caputine": 5.0}

    cb.update_params({"x_Local_Migration": xdict[catch]})



def catchment_ento(cb, catch):
    arab_times, arab_spline = catch_3_yr_spline(catch, "arabiensis")
    funest_times, funest_spline = catch_3_yr_spline(catch, "funestus")

    # Arabiensis
    set_species_param(cb,
                      'arabiensis',
                      'Larval_Habitat_Types',
                      {
                          "LINEAR_SPLINE": {
                              "Capacity_Distribution_Number_Of_Years": 3,
                              "Capacity_Distribution_Over_Time": {
                                  "Times": arab_times,
                                  "Values": arab_spline
                              },
                              "Max_Larval_Capacity": pow(10, 8.5)
                          }
                      })

    # Funestus
    set_species_param(cb,
                      'funestus',
                      'Larval_Habitat_Types',
                      {
                          "LINEAR_SPLINE": {
                              "Capacity_Distribution_Number_Of_Years": 3,
                              "Capacity_Distribution_Over_Time": {
                                  "Times": funest_times,
                                  "Values": funest_spline
                              },
                              "Max_Larval_Capacity": pow(10, 7.5)
                          }
                      })


def prev_ref_data(catch, start_date = '2009-01-01'):
    reference_csv = pd.read_csv(os.path.join(project_folder, "dtk_simulation_input/mozambique/grid_prevalence_with_dates.csv"))
    lookup_csv = pd.read_csv(os.path.join(project_folder, "dtk_simulation_input/mozambique/grid_lookup_with_neighborhood.csv"))
    return_data = reference_csv.merge(lookup_csv, how='left', on='grid_cell')

    return_data['sim_date'] = return_data['date'].apply(lambda x: convert_to_day_365(x, start_date))
    return_data['sim_date'] = return_data['sim_date'] - 1

    in_catch = return_data['catchment'] == catch
    return return_data[in_catch]

def incid_ref_data(catch, start_date = '2009-01-01'):
    reference_data = pd.read_csv(os.path.join(project_folder,"dtk_simulation_input/mozambique/catchment_incidence.csv")) #This one has been scaled down to pop of 10k
    # reference_data = pd.read_csv(os.path.join(project_folder,"dtk_simulation_input/mozambique/catchment_incidence_Mahel_test.csv"))
    reference_data.dropna(inplace=True)

    # Add year and month to dataframe based on fulldate column
    reference_data["year"] = reference_data['fulldate'].apply(lambda x: int(str(x).split('-')[0]))
    reference_data["month"] = reference_data['fulldate'].apply(lambda x: int(str(x).split('-')[1]))

    # Normalize by year for direct comparison with data:
    reference_data["year"] = reference_data["year"] - int(start_date.split('-')[0])

    in_catch = reference_data['catchment'] == catch
    return reference_data[in_catch]

def get_ref_data(catch, type, start_date='2009-01-01'):
    # Return data that respective LL analyzer needs, for specified catchment:
    if type == "prevalence":
        return prev_ref_data(catch, start_date=start_date)
    elif type == "incidence":
        return incid_ref_data(catch, start_date=start_date)



# Could also pass in analyzer list in future
class calib_site(CalibSite):
    def __init__(self, catch):
        super().__init__(catch)
        self.catch = catch

    def get_reference_data(self, type):
        return get_ref_data(self.catch, type)

    def get_analyzers(self):
        return [SpatialPrevalenceLikelihood(self.get_reference_data("prevalence"), weight=1),
                IncidenceLikelihood(self.get_reference_data("incidence"), weight=1)]